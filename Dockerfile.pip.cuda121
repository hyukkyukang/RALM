FROM hyukkyukang/ralm:py3.12-cuda-12.1.1-cudnn8-devel-ubuntu22.04


RUN pip install --upgrade pip && \
    pip install torch --index-url https://download.pytorch.org/whl/nightly/cu121 && pip cache purge

RUN pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl && pip cache purge

COPY requirements.txt /tmp/requirements.txt
RUN pip install -r /tmp/requirements.txt && \
    pip cache purge
