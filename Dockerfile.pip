FROM hyukkyukang/ralm:py3.13-cuda-12.8.1-cudnn-devel-ubuntu24.04

RUN pip install torch --index-url https://download.pytorch.org/whl/cu128 && pip cache purge
RUN pip install legacy-cgi && pip cache purge

RUN wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp313-cp313-linux_x86_64.whl \
 && pip install flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp313-cp313-linux_x86_64.whl \
 && rm flash_attn-2.7.4.post1+cu12torch2.6cxx11abiTRUE-cp313-cp313-linux_x86_64.whl && pip cache purge

COPY requirements.txt /tmp/requirements.txt
RUN pip install -r /tmp/requirements.txt && \
    pip cache purge