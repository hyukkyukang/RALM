name: adamw

# Optimizer parameters
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
warmup_steps: 2000
gradient_clip_val: 1.0
max_learning_rate: 4e-4
min_learning_rate: 4e-5
# lr_scheduler: cosine_decay
lr_scheduler: linear_decay