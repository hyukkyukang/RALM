# Optimization parameters
precision: 16-mixed
gradient_clip_val: 1.0
max_learning_rate: 4e-4
min_learning_rate: 4e-5
weight_decay: 0.1
warmup_steps: 2000
gradient_accumulation_steps: 2
per_device_train_batch_size: 2
max_epochs: 1

# Logging and checkpointing
logging_steps: 100
checkpoint_save_steps: 1000
resume_ckpt_path: Null

# Dataloader
train_num_workers: 8
use_torch_compile: True

