# Optimization parameters
precision: 16-mixed
gradient_clip_val: 1.0
max_learning_rate: 4e-4
min_learning_rate: 4e-5
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
warmup_steps: 2000
gradient_accumulation_steps: 16
per_device_train_batch_size: 9
max_epochs: 1

# Logging and checkpointing
logging_steps: 500
checkpoint_save_steps: 10000
resume_ckpt_path: Null

# Dataloader
train_num_workers: 8
use_torch_compile: True

