# Optimization parameters
precision: 16-mixed
gradient_clip_val: 1.0
learning_rate: 5e-5
weight_decay: 0.01
warmup_steps: 1000
gradient_accumulation_steps: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
max_epochs: 1

# Logging and checkpointing
logging_dir: None
logging_steps: 100
val_check_interval_by_step: 100
resume_ckpt_path: None

# Dataloader
train_num_workers: 1
val_num_workers: 1
test_num_workers: 1