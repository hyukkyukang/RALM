large:
  layers: 24
  num_attention_heads: 20
  num_key_value_heads: 5 # this should be factor of num_attention_heads
  intermediate_size: 5120
  hidden_size: 1280
  # We follow the GPT-2 architecture to decide the above parameters.
  # https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py?t